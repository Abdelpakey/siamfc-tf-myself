{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape==(2140, 96, 96, 1);train_data.min==0.000;train_data.max=1.000\n",
      "train_label.shape==(2140, 30);train_label.min==-0.920;train_label.max=0.996\n",
      "test_data.shape==(1783, 96, 96, 1);\n",
      "loss_begin\n",
      "begin_inference1\n",
      "begin_session\n",
      "0.0709779 0.0522845 0.0413359 0.035148 0.0315759 0.0311076 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "#'~/Documents/Machine Learning/Person homework/training.csv\n",
    "FTRAIN='training.csv'\n",
    "FTEST='test.csv'\n",
    "\n",
    "#将所有的图片的尺寸重现设置为32*32\n",
    "w=96\n",
    "h=96\n",
    "c=1\n",
    "\n",
    "\"\"\"Loads data from FTEST if *test* is True,otherwise from FTRAIN\n",
    "Padd a list of *cols* if you are only interested in a subset of the target columns\"\"\"\n",
    "def load(test=False,cols=None):\n",
    "    \n",
    "    fname=FTEST if test else FTRAIN\n",
    "    #load pandas dataframe\n",
    "    df=read_csv(os.path.expanduser(fname))\n",
    "    \n",
    "    #The Image column has piazxel values separated by space:convert\n",
    "    #the values to numpy arrays:\n",
    "    df['Image']=df['Image'].apply(lambda im: np.fromstring(im,sep=' '))\n",
    "    \n",
    "    #get a subset of columns\n",
    "    if cols:\n",
    "        df=df[list(cols)+['Image']]\n",
    "    \n",
    "    #print the number of values for each column\n",
    "    #print(df.count())\n",
    "    #drop all rows that have missing values in them\n",
    "    df=df.dropna()\n",
    "    \n",
    "    #scale pixel values to [0,1]\n",
    "    train_data=np.vstack(df['Image'].values)/255\n",
    "    train_data=train_data.astype(np.float32)\n",
    "    \n",
    "    #only FTRAIN has any target columns\n",
    "    if not test:\n",
    "        train_label=df[df.columns[:-1]].values\n",
    "        #scale target coordiates to [-1,1]\n",
    "        train_label=(train_label-48)/48\n",
    "        #shuffle train data\n",
    "        train_data,train_label=shuffle(train_data,train_label,random_state=42)\n",
    "        train_label=train_label.astype(np.float32)\n",
    "    else:\n",
    "        train_label=None\n",
    "        \n",
    "    return train_data,train_label\n",
    "\n",
    "def load2d(test=False,cols=None):\n",
    "    X,y=load(test=test)\n",
    "    #X=X.reshape(-1,1,96,96)\n",
    "    \n",
    "    X=X.reshape(-1,w,h,c)\n",
    "    return X,y\n",
    "\n",
    "def load_show(test=False,cols=None):\n",
    "    \n",
    "    fname=FTEST if test else FTRAIN\n",
    "    #load pandas dataframe\n",
    "    df=read_csv(os.path.expanduser(fname))\n",
    "    \n",
    "    #The Image column has piazxel values separated by space:convert\n",
    "    #the values to numpy arrays:\n",
    "    df['Image']=df['Image'].apply(lambda im: np.fromstring(im,sep=' '))\n",
    "    \n",
    "    #get a subset of columns\n",
    "    if cols:\n",
    "        df=df[list(cols)+['Image']]\n",
    "    \n",
    "    #print the number of values for each column\n",
    "    #print(df.count())\n",
    "    #drop all rows that have missing values in them\n",
    "    df=df.dropna()\n",
    "    \n",
    "    #scale pixel values to [0,1]\n",
    "    train_data=np.vstack(df['Image'].values)\n",
    "    train_data=train_data.astype(np.float32)\n",
    "    \n",
    "    #only FTRAIN has any target columns\n",
    "    if not test:\n",
    "        train_label=df[df.columns[:-1]].values\n",
    "        #scale target coordiates to [-1,1]\n",
    "        train_label=(train_label-48)/48\n",
    "        #shuffle train data\n",
    "        train_data,train_label=shuffle(train_data,train_label,random_state=42)\n",
    "        train_label=train_label.astype(np.float32)\n",
    "    else:\n",
    "        train_label=None\n",
    "        \n",
    "    return train_data,train_label\n",
    "\n",
    "def load2d_show(test=False,cols=None):\n",
    "    X,y=load_show(test=test)\n",
    "    #X=X.reshape(-1,1,96,96)\n",
    "    \n",
    "    X=X.reshape(-1,w,h,c)\n",
    "    return X,y\n",
    "\n",
    "#test load data\n",
    "train_data,train_label=load2d()\n",
    "test_data,test_label=load2d(test=True)\n",
    "print(\"train_data.shape=={};train_data.min=={:.3f};train_data.max={:.3f}\".format(train_data.shape,train_data.min(),train_data.max()))\n",
    "print(\"train_label.shape=={};train_label.min=={:.3f};train_label.max={:.3f}\".format(train_label.shape,train_label.min(),train_label.max()))\n",
    "print(\"test_data.shape=={};\".format(test_data.shape))\n",
    "\n",
    "\n",
    "#打乱训练数据及测试数据\n",
    "train_image_num=len(train_data)\n",
    "train_image_index=np.arange(train_image_num)\n",
    "np.random.shuffle(train_image_index)\n",
    "train_data=train_data[train_image_index]\n",
    "train_label=train_label[train_image_index]\n",
    "\n",
    "test_image_num=len(test_data)\n",
    "test_image_index=np.arange(test_image_num)\n",
    "np.random.shuffle(test_image_index)\n",
    "test_data=test_data[test_image_index]\n",
    "#test_label=test_label[test_image_index]\n",
    "\n",
    "keypoint_index={\n",
    "    'left_eye_center_x':0,\n",
    "    'left_eye_center_y':1,\n",
    "    'right_eye_center_x':2,\n",
    "    'right_eye_center_y':3,\n",
    "    'left_eye_inner_corner_x':4,\n",
    "    'left_eye_inner_corner_y':5,\n",
    "    'left_eye_outer_corner_x':6,\n",
    "    'left_eye_outer_corner_y':7,\n",
    "    'right_eye_inner_corner_x':8,\n",
    "    'right_eye_inner_corner_y':9,\n",
    "    'right_eye_outer_corner_x':10,\n",
    "    'right_eye_outer_corner_y':11,\n",
    "    'left_eyebrow_inner_end_x':12,\n",
    "    'left_eyebrow_inner_end_y':13,\n",
    "    'left_eyebrow_outer_end_x':14,\n",
    "    'left_eyebrow_outer_end_y':15,\n",
    "    'right_eyebrow_inner_end_x':16,\n",
    "    'right_eyebrow_inner_end_y':17,\n",
    "    'right_eyebrow_outer_end_x':18,\n",
    "    'right_eyebrow_outer_end_y':19,\n",
    "    'nose_tip_x':20,\n",
    "    'nose_tip_y':21,\n",
    "    'mouth_left_corner_x':22,\n",
    "    'mouth_left_corner_y':23,\n",
    "    'mouth_right_corner_x':24,\n",
    "    'mouth_right_corner_y':25,\n",
    "    'mouth_center_top_lip_x':26,\n",
    "    'mouth_center_top_lip_y':27,\n",
    "    'mouth_center_bottom_lip_x':28,\n",
    "    'mouth_center_bottom_lip_y':29\n",
    "}\n",
    "\n",
    "#搭建cnn\n",
    "#定义形参，在执行过程中被赋值\n",
    "#[None,30]None代表行不确定，列是30\n",
    "x=tf.placeholder(tf.float32,[None,w,h,c],name='x')\n",
    "y_=tf.placeholder(tf.float32,[None,30],name='y_')\n",
    "keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "def inference(input_tensor,train,regularizer):\n",
    "    \n",
    "    print(\"begin_inference1\")\n",
    "    #第一层：卷积层，filter（3*3*6），padding=0，stride=1\n",
    "    #scale change:96*96*1->94*94*32\n",
    "    #返回一个对象\n",
    "    #with tf.variable_scope('layer1-conv1',reuse=True):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        #truncated_normal_initializer从截断的正态分布中输出随机值。(stddev :方差)\n",
    "        #[3,3,c,32]:[filter_size_w,filter_size_h,old_c_num,new_c_num]\n",
    "        conv1_weights=tf.get_variable('weight',[3,3,c,32],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases=tf.get_variable('bias',[32],initializer=tf.constant_initializer(0.0))\n",
    "        #tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "        conv1=tf.nn.conv2d(input_tensor,conv1_weights,strides=[1,1,1,1],padding='VALID')\n",
    "        #tf.nn.relu(features, name=None)\n",
    "        relu1=tf.nn.relu(tf.nn.bias_add(conv1,conv1_biases))\n",
    "        \n",
    "    #第二层：池化层，filter（2×2），paddig=0，stride=2\n",
    "    #scale change:94*94*32->47*47*32\n",
    "    with tf.name_scope('layer2_pool1'):\n",
    "        pool1=tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "    #第三层：卷积层，filter（2×2×64），padding=0，stride=1\n",
    "    #scale change:47*47*32->46*46*64\n",
    "    #返回一个对象\n",
    "    #with tf.variable_scope('layer3-conv2',reuse=True):\n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        #truncated_normal_initializer从截断的正态分布中输出随机值。(stddev :方差)\n",
    "        conv2_weights=tf.get_variable('weight',[2,2,32,64],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases=tf.get_variable('bias',[64],initializer=tf.constant_initializer(0.0))\n",
    "        #tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "        conv2=tf.nn.conv2d(pool1,conv2_weights,strides=[1,1,1,1],padding='VALID')\n",
    "        #tf.nn.relu(features, name=None)\n",
    "        relu2=tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases))\n",
    "        \n",
    "     #第四层：池化层，filter（2×2），paddig=0，stride=2\n",
    "    #scale change:46*46*64->23*23*64\n",
    "    with tf.name_scope('layer4_pool2'):\n",
    "        pool2=tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "     #第五层：卷积层，filter（2×2×128），padding=0，stride=1\n",
    "    #scale change:23*23*64->22*22*128\n",
    "    #返回一个对象\n",
    "    #with tf.variable_scope('layer3-conv2',reuse=True):\n",
    "    with tf.variable_scope('layer5-conv3'):\n",
    "        #truncated_normal_initializer从截断的正态分布中输出随机值。(stddev :方差)\n",
    "        conv3_weights=tf.get_variable('weight',[2,2,64,128],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv3_biases=tf.get_variable('bias',[128],initializer=tf.constant_initializer(0.0))\n",
    "        #tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "        conv3=tf.nn.conv2d(pool2,conv3_weights,strides=[1,1,1,1],padding='VALID')\n",
    "        #tf.nn.relu(features, name=None)\n",
    "        relu3=tf.nn.relu(tf.nn.bias_add(conv3,conv3_biases))\n",
    "        \n",
    "     #第六层：池化层，filter（2×2），paddig=0，stride=2\n",
    "    #scale change:22*22*128->11*11*128\n",
    "    with tf.name_scope('layer6_pool3'):\n",
    "        pool3=tf.nn.max_pool(relu3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    #layer6_pool3(11*11*128=15488)->full_connected_layer(一维向量 15488)\n",
    "    pool_shape=pool3.get_shape().as_list()\n",
    "    nodes=pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped=tf.reshape(pool3,[-1,nodes])\n",
    "    \n",
    "    #第七层：全连接层：nodes=11*11*128=15488，15488->500\n",
    "    #是否引入dropout，此过程中未引入\n",
    "    #with tf.variable_scope('layer7-fc1',reuse=True):\n",
    "    with tf.variable_scope('layer7-fc1'):\n",
    "        fc1_weights=tf.get_variable('weight',[nodes,500],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection('losses',regularizer(fc1_weights))\n",
    "        fc1_biases=tf.get_variable('bias',[500],initializer=tf.constant_initializer(0.1))\n",
    "        #tf.matmul()矩阵乘\n",
    "        fc1=tf.nn.relu(tf.matmul(reshaped,fc1_weights)+fc1_biases)\n",
    "        if train:\n",
    "            fc1=tf.nn.dropout(fc1,0.5)\n",
    "    \n",
    "    #第八层：全连接层：500->500\n",
    "    #是否引入dropout，此过程中未引入\n",
    "    #with tf.variable_scope('layer8-fc2',reuse=True):\n",
    "    with tf.variable_scope('layer8-fc2'):\n",
    "        fc2_weights=tf.get_variable('weight',[500,500],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection('losses',regularizer(fc2_weights))\n",
    "        fc2_biases=tf.get_variable('bias',[500],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        fc2=tf.nn.relu(tf.matmul(fc1,fc2_weights)+fc2_biases)\n",
    "        if train:\n",
    "            fc2=tf.nn.dropout(fc2,0.5)\n",
    "    \n",
    "    #第九层：全连接层：500->30\n",
    "    #get the final classification result\n",
    "    #with tf.variable_scope('layer7-fc3',reuse=True):\n",
    "    with tf.variable_scope('layer9-fc3'):\n",
    "        fc3_weights=tf.get_variable('weight',[500,30],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection('losses',regularizer(fc3_weights))\n",
    "        fc3_biases=tf.get_variable('bias',[30],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        logit=tf.matmul(fc2,fc3_weights)+fc3_biases\n",
    "        #??\n",
    "        rmse=tf.sqrt(tf.reduce_mean(tf.square(y_-logit)))\n",
    "        \n",
    "    return logit,rmse\n",
    "\n",
    "def read_from_file(filename):\n",
    "    with open(filename) as fd:\n",
    "        lines = fd.readlines()\n",
    "        \n",
    "    return lines\n",
    "\n",
    "#每次获取batch_size个样本进行训练\n",
    "def get_train_batch(data,label,batch_size):\n",
    "    #range范围[0,len（data）-batch_size]，每隔batch_size取一个数据\n",
    "    for start_index in range(0,len(data)-batch_size+1,batch_size):\n",
    "        #设置一个batch_size大小的切片\n",
    "        slice_index=slice(start_index,start_index+batch_size)\n",
    "        #yield暂停循环，完成后返回循环\n",
    "        yield data[slice_index],label[slice_index]\n",
    "        \n",
    "#每次获取batch_size个样本进行测试\n",
    "def get_test_batch(data,batch_size):\n",
    "    #range范围[0,len（data）-batch_size]，每隔batch_size取一个数据\n",
    "    for start_index in range(0,len(data)-batch_size+1,batch_size):\n",
    "        #设置一个batch_size大小的切片\n",
    "        slice_index=slice(start_index,start_index+batch_size)\n",
    "        #yield暂停循环，完成后返回循环\n",
    "        yield data[slice_index]\n",
    "        \n",
    "        \n",
    "#在直接执行脚本时调用，import时不用调用\n",
    "#if __name__=='__main__':\n",
    "print(\"loss_begin\")\n",
    "#正则化\n",
    "regularizer=tf.contrib.layers.l2_regularizer(0.001)\n",
    "y,loss=inference(x,False,regularizer)\n",
    "train_op=tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#创建会话\n",
    "sess=tf.InteractiveSession()\n",
    "print(\"begin_session\")\n",
    "#初始化所有的变量\n",
    "sess.run(tf.global_variables_initializer())train_op\n",
    "    \n",
    "    \n",
    "#train_process\n",
    "#将所有的样本训练10次，每次训练中以64次为一组\n",
    "train_num=1000\n",
    "batch_size=64\n",
    "validation_size=100\n",
    "EARLY_STOP_PATIENCE=20\n",
    "best_validation_loss=1000000.0\n",
    "current_epoch=0\n",
    "#确定验证集\n",
    "valid_data,valid_label=train_data[:validation_size],train_label[:validation_size]\n",
    "#出去验证集后的训练集\n",
    "_train_data,_train_label=train_data[validation_size:],train_label[validation_size:]\n",
    "\n",
    "for i in range(train_num):\n",
    "    #print(\"epoch:=={}\".format(i))\n",
    "              \n",
    "    train_loss,train_acc,batch_num=0,0,0\n",
    "    for train_data_batch,train_label_batch in get_train_batch(train_data,train_label,batch_size):\n",
    "        #sess.run(train_op,feed_dict={x:train_data_batch,y_:train_label_batch,keep_prob:0.5})\n",
    "        train_op.run(feed_dict={x:train_data_batch,y_:train_label_batch,keep_prob:0.5})\n",
    "        \n",
    "        #validation_loss=loss.eval(feed_dict={x:train_data_batch,y_:train_label_batch,keep_prob:1.0})\n",
    "        validation_loss=loss.eval(feed_dict={x:train_data_batch,y_:train_label_batch,keep_prob:1.0})\n",
    "        \n",
    "        if validation_loss < best_validation_loss: \n",
    "            best_validation_loss=validation_loss\n",
    "            cureent_epoch=i\n",
    "        elif (i-current_epoch)>=EARLY_STOP_PATIENCE:\n",
    "            #print ('early stopping')\n",
    "            break\n",
    "    print(best_validation_loss,end='')\n",
    "    print(' ',end='')\n",
    "        \n",
    "y_pred=[]\n",
    "    \n",
    "test_loss,test_acc,batch_num=0,0,0\n",
    "test_data_num=test_data.shape[0]\n",
    "    \n",
    "for test_data_batch in get_test_batch(test_data,batch_size):\n",
    "    y_batch=y.eval(feed_dict={x:test_data_batch,keep_prob:1.0})\n",
    "    y_pred.extend(y_batch)\n",
    "    \n",
    "    #如果测试数据不是batch_size的整数倍\n",
    "if (test_data_num % batch_size) != 0:\n",
    "    len_tmp=len(y_pred)\n",
    "    for j in range(len_tmp,test_data_num):\n",
    "        test_per=test_data[j,:]\n",
    "        test_per=test_per.reshape(1,w,h,c)\n",
    "        y_batch=y.eval(feed_dict={x:test_per,keep_prob:1.0})\n",
    "        y_pred.extend(y_batch)\n",
    "        \n",
    "print('predict test image done!')\n",
    "    \n",
    "output_file=open('submission4.csv','w')\n",
    "    \n",
    "IdLookupTable=open('IdLookupTable.csv')\n",
    "IdLookupTable.readline()\n",
    "    \n",
    "#output_file.write('{0},{1},{2},{3}\\n'.format(\"RowId\",\"ImageId\",\"FeatureName\",\"Location\"))\n",
    "output_file.write('{0},{1}\\n'.format(\"RowId\",\"Location\"))\n",
    "    \n",
    "for line in IdLookupTable:\n",
    "    RowId,ImageId,FeatureName=line.rstrip().split(',')\n",
    "    #print(\"RowId:{},ImageId:{},FeatureName:{}\".format(RowId,ImageId,FeatureName))\n",
    "    image_index=int(ImageId)-1\n",
    "    feature_index=keypoint_index[FeatureName]\n",
    "    feature_location=y_pred[image_index][feature_index]*48+48\n",
    "    #output_file.write('{0},{1},{2},{3}\\n'.format(RowId,ImageId,FeatureName,feature_location))\n",
    "    output_file.write('{0},{1}\\n'.format(RowId,feature_location))\n",
    "\n",
    "output_file.close()\n",
    "IdLookupTable.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data,test_label1=load2d_show(test=True)\n",
    "#test_data_num = 1\n",
    "for i in range(test_data_num):\n",
    "    print(\"image_show\")\n",
    "    test_img=test_data[i,:]\n",
    "    test_label=y_pred[i]\n",
    "    print(test_img)\n",
    "    img2 = np.asarray(test_img, dtype=np.uint8)\n",
    "    k=0\n",
    "    for j in range(15):\n",
    "        img2[int(test_label[k+1]*48+48)][int(test_label[k]*48+48)]=255\n",
    "        #test_img=test_img[int(test_label[k+1]*48+48)][int(test_label[k]*48+48)]=255\n",
    "        k=k+2\n",
    "    cv2.imshow(\"image_test\",img2)\n",
    "    cv2.imshow(\"image_test_uint8\",np.asarray(img2, dtype=np.uint8))\n",
    "    cv2.imwrite(\"img.bmp\",img2)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
